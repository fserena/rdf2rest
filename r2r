#!/usr/bin/env python
"""
#-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=#
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

            http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
#-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=#
"""

import json
import logging
import netifaces as ni
import shutil
import sys
from threading import Thread
from urlparse import urljoin

import os
import requests as requests
from rdflib import ConjunctiveGraph, URIRef

__author__ = 'Fernando Serena'

# Setup logging
log_level = int(os.environ.get('LOG_LEVEL', logging.INFO))
ch = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
ch.setLevel(log_level)
logger = logging.getLogger('rdf2rest')
logger.addHandler(ch)
logger.setLevel(log_level)

from rdf2rest.api import service_graph, NAMESPACES, app, SERVICE_TYPE
from rdf2rest.dataset import create_link_partition, create_type_partition, load_dataset, PARTITION

log = logging.getLogger('rdf2rest.r2r')


# Load environment variables
try:
    AGORA_HOST = os.environ.get('AGORA_HOST', 'localhost')
    AGORA_PORT = int(os.environ.get('AGORA_PORT', 9002))
    API_PORT = int(os.environ.get('API_PORT', 5001))
    SOURCE_FILE = os.environ.get('SOURCE_FILE', '')
    SOURCE_GRAPH_STORE = os.environ.get('SOURCE_GRAPH_STORE', 'source')
    PARTITION_GRAPH_STORE = os.environ.get('PARTITION_GRAPH_STORE', 'service')
    PARTITION_FILE = os.environ.get('PARTITION_FILE', 'partition.ttl')
    TYPE_PARTITION_URI = URIRef(os.environ.get('TYPE_PARTITION_URI', ''))
    LINK_PARTITION_URI = URIRef(os.environ.get('LINK_PARTITION_URI', ''))
    PARTITION_LIMIT = max(0, int(os.environ.get('PARTITION_LIMIT', 10)))
    PARTITION_OFFSET = max(0, int(os.environ.get('PARTITION_OFFSET', 0)))
    CREATE_PARTITION = json.loads(os.environ.get('CREATE_PARTITION', 'false'))
    LOAD_SOURCE = json.loads(os.environ.get('LOAD_SOURCE', 'false'))
    LOAD_PARTITION = json.loads(os.environ.get('LOAD_PARTITION', 'false'))
    SERVICE_VOCABULARY_URL = os.environ.get('SERVICE_VOCABULARY_URL', '')
    SERVICE_VOCABULARY_ID = os.environ.get('SERVICE_VOCABULARY_ID', '')
    IGNORE_LINKS = json.dumps(os.environ.get('IGNORE_LINKS', '[]'))
except Exception, e:
    log.error(e.message)
    sys.exit(-1)

if LOAD_SOURCE and not SOURCE_FILE:
    log.error('Cannot load source if the dataset is not specified')
    sys.exit(0)

if CREATE_PARTITION and not TYPE_PARTITION_URI and not LINK_PARTITION_URI:
    log.error('Cannot create a partition when both type and link URIs are not defined')
    sys.exit(0)


def batch_create():
    if TYPE_PARTITION_URI:
        print 'A partition of {} {} resources is about to be created...\n'.format(PARTITION_LIMIT, TYPE_PARTITION_URI)
        create_type_partition(source_graph, service_graph, TYPE_PARTITION_URI, limit=PARTITION_LIMIT,
                              offset=PARTITION_OFFSET, file_name=PARTITION_FILE, ignore=IGNORE_LINKS)
    elif LINK_PARTITION_URI:
        print 'A partition of {} resources linked by {} is about to be created...\n'.format(PARTITION_LIMIT,
                                                                                            LINK_PARTITION_URI)
        create_link_partition(source_graph, service_graph, LINK_PARTITION_URI, limit=PARTITION_LIMIT,
                              offset=PARTITION_OFFSET, file_name=PARTITION_FILE, ignore=IGNORE_LINKS)


def service_uri():
    eth0 = ni.ifaddresses('eth0')
    try:
        return eth0[2][0]['addr']
    except Exception:
        pass

    wlan0 = ni.ifaddresses('wlan0')
    try:
        return wlan0[2][0]['addr']
    except Exception:
        pass

    log.error('There are no network interfaces available')
    sys.exit(-1)


if __name__ == '__main__':
    print '-           RDF2REST v0.0.1             -'
    print '- by Fernando Serena [fserena @ GitHub] -\n'
    service_graph.open(PARTITION_GRAPH_STORE, create=True)

    AGORA_URL = 'http://{}:{}'.format(AGORA_HOST, AGORA_PORT)

    print 'Getting known vocabularies from Agora...',
    try:
        known_vocabs = requests.get(urljoin(AGORA_URL, 'vocabs')).json()
    except Exception, e:
        log.error(e.message)
        sys.exit(-1)
    print 'Done.'

    # POST/PUT bank vocabulary
    with open(SERVICE_VOCABULARY_URL) as f:
        ttl = f.read()
        if SERVICE_VOCABULARY_ID not in known_vocabs:
            print "'{}' vocabulary is not present in the Agora, teaching...".format(SERVICE_VOCABULARY_ID),
            response = requests.post(urljoin(AGORA_URL, 'vocabs'), data=ttl, headers={'Content-Type': 'text/turtle'})
            if response.status_code == 201:
                print 'Done.'
            else:
                print 'Error.'
        else:
            print "Updating '{}' vocabulary...".format(SERVICE_VOCABULARY_ID),
            response = requests.put(urljoin(AGORA_URL, 'vocabs/{}'.format(SERVICE_VOCABULARY_ID)), data=ttl,
                                    headers={'Content-Type': 'text/turtle'})
            if response.status_code == 200:
                print 'Done.'
            else:
                print 'Error.'

    prefixed_service_type = service_graph.namespace_manager.qname(SERVICE_TYPE)
    suri = 'http://{}:{}/'.format(service_uri(), API_PORT)
    print 'Self-registering as seed with URI {}...'.format(suri),
    response = requests.post(urljoin(AGORA_URL, 'seeds'),
                             data=json.dumps(
                                 {'type': prefixed_service_type, 'uri': suri}),
                             headers={'Content-Type': 'application/json'})
    if response.status_code == 201 or response.status_code == 409:
        print 'Done.'
    else:
        print 'Error: {}.'.format(response.status_code)

    source_graph = ConjunctiveGraph('Sleepycat')

    if LOAD_SOURCE:
        print 'Initializing source graph...',
        if os.path.exists(SOURCE_GRAPH_STORE):
            shutil.rmtree(SOURCE_GRAPH_STORE)
        print 'Done.'
        print 'Loading source graph...',
        load_dataset(source_graph, SOURCE_GRAPH_STORE, SOURCE_FILE, blocking=True)
        print 'Done.'
    else:
        print 'Opening source graph...',
        source_graph.open(SOURCE_GRAPH_STORE, create=True)
        print 'Done.'

    if CREATE_PARTITION:
        print 'Preparing to create a dataset partition...'

        print 'Updating source graph namespaces...',
        for prefix in NAMESPACES:
            source_graph.bind(prefix, NAMESPACES[prefix])
        print 'Done.'

        print 'Initializing partition graph...',
        if os.path.exists(PARTITION_GRAPH_STORE):
            shutil.rmtree(PARTITION_GRAPH_STORE)
        service_graph.open(PARTITION_GRAPH_STORE, create=True)
        print 'Done.'
        batch_thread = Thread(target=batch_create)
        batch_thread.daemon = True
        batch_thread.start()

    else:
        if LOAD_PARTITION or not list(service_graph.triples((None, None, PARTITION.Root))):
            print "Starting update of partition dataset from file {}...".format(PARTITION_FILE)
            load_dataset(service_graph, PARTITION_GRAPH_STORE, PARTITION_FILE)

    print '\nStarting REST API...'
    app.run(host='0.0.0.0', debug=True, port=API_PORT, use_reloader=False, threaded=True)
